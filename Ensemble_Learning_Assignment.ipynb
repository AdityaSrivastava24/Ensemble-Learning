{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical"
      ],
      "metadata": {
        "id": "_KsO6k1hx4uW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.Can we use Bagging for regression problems?\n",
        "\n",
        "->Yes, **Bagging (Bootstrap Aggregating)** can definitely be used for **regression problems**.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ What is Bagging?\n",
        "\n",
        "Bagging is an ensemble technique that:\n",
        "\n",
        "* Trains multiple models (usually of the same type) on **different random subsets** (with replacement) of the training data.\n",
        "* Averages their predictions (for regression) or uses majority voting (for classification) to improve performance.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ How Bagging Works for Regression:\n",
        "\n",
        "* Each base model (e.g., a Decision Tree Regressor) is trained on a bootstrap sample.\n",
        "* Final prediction is the **average of all base regressors' predictions**.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Example in Python (Using `BaggingRegressor` from `sklearn`):\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_boston(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging with Decision Trees\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Benefits of Using Bagging for Regression:\n",
        "\n",
        "* Reduces **variance**, making predictions more stable.\n",
        "* Helps prevent **overfitting**, especially with high-variance models like decision trees.\n",
        "* Works well even when individual models are not very strong.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Popular Bagging-Based Regressors:\n",
        "\n",
        "* **Random Forest Regressor** (bagging of decision trees with feature randomness)\n",
        "* **BaggingRegressor** in `sklearn`\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wS8VaAj0yu_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6LfPqSy0y-sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.What is the difference between multiple model training and single model training?\n",
        "->The key difference between **multiple model training** and **single model training** lies in how many models are trained and how their predictions are used. Here's a clear breakdown:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 1. **Single Model Training**\n",
        "\n",
        "* **What it is:** You train **one machine learning model** on the entire training dataset.\n",
        "* **Examples:** Linear Regression, Decision Tree, SVM, etc., trained once.\n",
        "\n",
        "#### ✅ Pros:\n",
        "\n",
        "* Simpler to train and interpret.\n",
        "* Faster and uses less memory.\n",
        "* Easier to deploy.\n",
        "\n",
        "#### ❌ Cons:\n",
        "\n",
        "* May **overfit or underfit** depending on model complexity.\n",
        "* Can be sensitive to noise or outliers.\n",
        "* Limited by the model’s inherent strengths/weaknesses.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 2. **Multiple Model Training (Ensemble Learning)**\n",
        "\n",
        "* **What it is:** You train **several models**, then combine their predictions.\n",
        "* **Techniques:**\n",
        "\n",
        "  * **Bagging** (e.g., Random Forest)\n",
        "  * **Boosting** (e.g., Gradient Boosting, AdaBoost)\n",
        "  * **Stacking**\n",
        "  * **Voting ensembles**\n",
        "\n",
        "#### ✅ Pros:\n",
        "\n",
        "* Usually **more accurate and robust** than single models.\n",
        "* Can **reduce variance** (e.g., Bagging) or **reduce bias** (e.g., Boosting).\n",
        "* Works well in competitions and real-world systems.\n",
        "\n",
        "#### ❌ Cons:\n",
        "\n",
        "* **Complex and computationally expensive**.\n",
        "* Harder to interpret.\n",
        "* Slower training and inference.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 Key Comparison\n",
        "\n",
        "| Aspect           | Single Model            | Multiple Models (Ensemble)       |\n",
        "| ---------------- | ----------------------- | -------------------------------- |\n",
        "| Model Count      | 1                       | Multiple                         |\n",
        "| Performance      | Depends on model & data | Usually better generalization    |\n",
        "| Training Time    | Faster                  | Slower                           |\n",
        "| Interpretability | Easier to interpret     | Harder to interpret              |\n",
        "| Robustness       | Less robust to noise    | More robust                      |\n",
        "| Examples         | Linear Regression, SVM  | Random Forest, Gradient Boosting |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary\n",
        "\n",
        "* Use **single model training** when you need simplicity, speed, and interpretability.\n",
        "* Use **multiple model training (ensembles)** when you want **better performance and robustness**, especially for complex tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "DjAzmrqazB6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Explain the concept of feature randomness in Random Forest\n",
        "\n",
        "->### 🌲 Feature Randomness in Random Forest – Explained\n",
        "\n",
        "**Random Forest** is an ensemble learning method that combines the predictions of multiple decision trees to improve accuracy and control overfitting.\n",
        "\n",
        "One of the key reasons Random Forest performs better than a single decision tree is **feature randomness** (also called **feature bagging** or **attribute sampling**).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 What is Feature Randomness?\n",
        "\n",
        "In **Random Forest**, when each tree is being constructed:\n",
        "\n",
        "* At **each split in the tree**, only a **random subset of features** is considered to find the best split.\n",
        "* This is different from a regular decision tree, which considers **all features** at every split.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Why is Feature Randomness Important?\n",
        "\n",
        "1. **Reduces Correlation Between Trees:**\n",
        "\n",
        "   * If all trees used the same most predictive features, they’d be highly correlated.\n",
        "   * Feature randomness makes the trees **diverse**, improving the ensemble's performance.\n",
        "\n",
        "2. **Improves Generalization:**\n",
        "\n",
        "   * Diversity in trees helps Random Forest avoid overfitting, especially on noisy datasets.\n",
        "\n",
        "3. **Increases Model Robustness:**\n",
        "\n",
        "   * The model does not rely too heavily on a few strong predictors, making it more balanced.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Example:\n",
        "\n",
        "Assume you have a dataset with 10 features: `F1, F2, ..., F10`.\n",
        "\n",
        "* In a standard decision tree, all 10 features are evaluated at every split.\n",
        "* In a Random Forest:\n",
        "\n",
        "  * If using `max_features=3`, then **only 3 randomly chosen features** (e.g., `F2, F7, F9`) are considered at that split.\n",
        "  * This selection is repeated **independently** at each node in every tree.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 In `scikit-learn`:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_features='sqrt',  # or 'log2', or an int/fraction\n",
        "    random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "* Common choices for `max_features`:\n",
        "\n",
        "  * `'sqrt'`: square root of total features (default for classification)\n",
        "  * `'log2'`: log2 of total features\n",
        "  * A fixed number or float (e.g., `0.5` means 50% of features)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary:\n",
        "\n",
        "| Aspect                      | Random Forest (with Feature Randomness)     |\n",
        "| --------------------------- | ------------------------------------------- |\n",
        "| Feature selection per split | Random subset of features                   |\n",
        "| Benefit                     | Reduces correlation, improves accuracy      |\n",
        "| Common parameter            | `max_features`                              |\n",
        "| Key idea                    | **Inject randomness to reduce overfitting** |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mPd_Qn09zSi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "->### 🧪 What is OOB (Out-of-Bag) Score in Random Forest?\n",
        "\n",
        "**OOB (Out-of-Bag) Score** is a performance metric used to evaluate **Random Forest** models **without needing a separate validation set**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 How It Works:\n",
        "\n",
        "Random Forest uses **Bagging** (Bootstrap Aggregating), which:\n",
        "\n",
        "* Trains each tree on a **random sample with replacement** from the training data.\n",
        "* On average, each tree sees about **63%** of the training samples.\n",
        "* The remaining **\\~37%** of the data (not used in training that tree) is called the **Out-of-Bag data** for that tree.\n",
        "\n",
        "Each data point is likely to be **out-of-bag** for about **1/3 of the trees**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 OOB Prediction:\n",
        "\n",
        "* For each sample, you use only the trees **where it was not included in training** to predict its output.\n",
        "* You then compare these OOB predictions with the actual values.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 OOB Score:\n",
        "\n",
        "* **For classification**: It’s the **accuracy** of OOB predictions.\n",
        "* **For regression**: It’s usually the **R² score** (coefficient of determination) based on OOB predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 In `scikit-learn`:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print(\"OOB Score:\", model.oob_score_)\n",
        "```\n",
        "\n",
        "> Note: `bootstrap=True` is required for `oob_score` to work (default is `True`).\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Advantages of OOB Score:\n",
        "\n",
        "| Advantage           | Description                                   |\n",
        "| ------------------- | --------------------------------------------- |\n",
        "| Efficient           | No need for a separate validation set         |\n",
        "| Built-in            | Automatically computed during training        |\n",
        "| Reliable for tuning | Gives a good estimate of generalization error |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary:\n",
        "\n",
        "| Term           | Meaning                                                           |\n",
        "| -------------- | ----------------------------------------------------------------- |\n",
        "| OOB Sample     | Data not seen by a particular tree                                |\n",
        "| OOB Prediction | Prediction made using only trees that didn’t see the sample       |\n",
        "| OOB Score      | Accuracy (classification) or R² (regression) from OOB predictions |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5M1-xk-zcd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "->You can **measure feature importance** in a **Random Forest** model by analyzing how much each feature contributes to reducing impurity or improving model performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Two Main Methods for Measuring Feature Importance in Random Forest:\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 1. **Mean Decrease in Impurity (MDI)** – *Gini Importance*\n",
        "\n",
        "* This is the **default method** used in `scikit-learn`.\n",
        "* When a feature is used to split a node, the impurity (like Gini or MSE) is reduced.\n",
        "* This **reduction is accumulated** over all trees and all nodes where the feature is used.\n",
        "* More reduction = more important.\n",
        "\n",
        "#### 🔧 In `scikit-learn`:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance scores\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Combine with feature names\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 2. **Mean Decrease in Accuracy (MDA)** – *Permutation Importance*\n",
        "\n",
        "* This is a **model-agnostic method**.\n",
        "* Idea: Randomly shuffle one feature’s values and see how much model accuracy drops.\n",
        "* Bigger drop = more important feature.\n",
        "\n",
        "#### 🔧 In `scikit-learn`:\n",
        "\n",
        "```python\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "sorted_idx = result.importances_mean.argsort()\n",
        "\n",
        "# Feature importance based on drop in model score\n",
        "for i in sorted_idx[::-1]:\n",
        "    print(f\"{X_test.columns[i]}: {result.importances_mean[i]:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Comparison:\n",
        "\n",
        "| Method            | Based On                         | Pros                          | Cons                                    |\n",
        "| ----------------- | -------------------------------- | ----------------------------- | --------------------------------------- |\n",
        "| MDI (default)     | Impurity reduction during splits | Fast, built-in                | Biased toward high-cardinality features |\n",
        "| MDA (Permutation) | Model performance drop           | More accurate, model-agnostic | Slower, needs more computation          |\n",
        "\n",
        "---\n",
        "\n",
        "### 📈 Bonus: Visualization\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot top features\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
        "plt.title(\"Feature Importance in Random Forest\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary\n",
        "\n",
        "* **MDI (Gini importance):** Fast and default, based on impurity decrease.\n",
        "* **MDA (Permutation):** Slower but more reliable, based on accuracy drop.\n",
        "* Choose based on your need for **speed vs interpretability**.\n",
        "\n"
      ],
      "metadata": {
        "id": "ll6ZS57jzrmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.Explain the working principle of a Bagging Classifier\n",
        "\n",
        "->### 🧠 **Working Principle of a Bagging Classifier (Bootstrap Aggregating Classifier)**\n",
        "\n",
        "**Bagging** (short for **Bootstrap Aggregating**) is an ensemble learning technique that builds **multiple versions of a classifier** and combines them to produce better performance—especially by reducing **variance** and **overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 How a Bagging Classifier Works – Step-by-Step:\n",
        "\n",
        "#### 🧩 1. **Bootstrap Sampling**\n",
        "\n",
        "* From the original training data (of size `N`), create **multiple random subsets** (also of size `N`) by sampling **with replacement**.\n",
        "* Some points may appear multiple times; others may be excluded in a given subset.\n",
        "\n",
        "#### 🌳 2. **Train Base Models**\n",
        "\n",
        "* Train a separate classifier (e.g., Decision Tree) on **each bootstrap sample**.\n",
        "* Each model learns slightly differently due to data variation.\n",
        "\n",
        "#### 📊 3. **Aggregate Predictions**\n",
        "\n",
        "* For **classification**: Use **majority voting** across all classifiers.\n",
        "* For **regression** (Bagging Regressor): Use **average** of predictions.\n",
        "\n",
        "#### 🔄 4. **Out-of-Bag (OOB) Evaluation (optional)**\n",
        "\n",
        "* Each training sample is “out-of-bag” (not used) in \\~37% of models.\n",
        "* You can evaluate model accuracy using only the models where the sample was not included.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Example in `scikit-learn` (Classification):\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=10,\n",
        "    oob_score=True,       # Enable OOB scoring\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train and predict\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"OOB Score:\", bagging_clf.oob_score_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Advantages of Bagging Classifier:\n",
        "\n",
        "| Benefit                  | Explanation                                  |\n",
        "| ------------------------ | -------------------------------------------- |\n",
        "| Reduces variance         | Aggregation smooths out predictions          |\n",
        "| Handles overfitting      | Especially helpful with high-variance models |\n",
        "| Robust to noise          | Random sampling reduces impact of outliers   |\n",
        "| Supports parallelization | Models are trained independently             |\n",
        "\n",
        "---\n",
        "\n",
        "### ❌ Limitations:\n",
        "\n",
        "* Doesn’t reduce **bias** (ineffective if base models are too simple).\n",
        "* Less effective for **low-variance** models (like linear classifiers).\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Summary Table:\n",
        "\n",
        "| Step        | Description                                            |\n",
        "| ----------- | ------------------------------------------------------ |\n",
        "| Sampling    | Randomly sample with replacement (bootstrap)           |\n",
        "| Training    | Train each classifier on a different subset            |\n",
        "| Aggregation | Majority vote (classification) or average (regression) |\n",
        "| Key Benefit | Reduces **variance** to improve generalization         |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "usHQOKHX04sQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.How do you evaluate a Bagging Classifier’s performance?\n",
        "\n",
        "->To **evaluate a Bagging Classifier's performance**, you use the same techniques as for any supervised classification model—measuring how well it predicts on unseen data. However, Bagging has some **unique evaluation options** as well. Here's a complete guide:\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **1. Accuracy on Test/Validation Data**\n",
        "\n",
        "* Measure the percentage of correctly classified samples.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **2. Confusion Matrix**\n",
        "\n",
        "* Gives a detailed breakdown of true positives, false positives, etc.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **3. Classification Report**\n",
        "\n",
        "* Includes precision, recall, F1-score, and support for each class.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **4. Cross-Validation**\n",
        "\n",
        "* Use k-fold cross-validation to evaluate generalization across different data splits.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(bagging_model, X, y, cv=5)\n",
        "print(\"Cross-validation Accuracy: %.2f%% (+/- %.2f%%)\" % (scores.mean()*100, scores.std()*100))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **5. ROC Curve and AUC (for binary classification)**\n",
        "\n",
        "* Evaluates the model’s ability to separate classes.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "probs = bagging_model.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, probs)\n",
        "fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "\n",
        "plt.plot(fpr, tpr, label=\"AUC = %.2f\" % auc)\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **6. Out-of-Bag (OOB) Score (specific to Bagging)**\n",
        "\n",
        "* Uses \"out-of-bag\" samples to estimate performance **without needing a validation set**.\n",
        "* Only works if `oob_score=True` and `bootstrap=True` during model creation.\n",
        "\n",
        "```python\n",
        "print(\"OOB Score:\", bagging_model.oob_score_)\n",
        "```\n",
        "\n",
        "* This gives a good internal estimate of test accuracy during training.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary Table\n",
        "\n",
        "| Metric                    | Use Case                                        |\n",
        "| ------------------------- | ----------------------------------------------- |\n",
        "| **Accuracy**              | Overall correctness                             |\n",
        "| **Confusion Matrix**      | Detailed class-by-class error breakdown         |\n",
        "| **Classification Report** | Precision, recall, F1 for each class            |\n",
        "| **Cross-Validation**      | More reliable estimate of model performance     |\n",
        "| **ROC AUC**               | Performance on imbalanced/binary problems       |\n",
        "| **OOB Score**             | Built-in validation using unused bootstrap data |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2aPiqTih1Hco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. How does a Bagging Regressor work?\n",
        "\n",
        "->### 🧠 How Does a **Bagging Regressor** Work?\n",
        "\n",
        "A **Bagging Regressor** is an ensemble method that improves the performance of regression models by **reducing variance** through **bootstrap aggregation (bagging)**.\n",
        "\n",
        "It works by training multiple regressors on random subsets of the data and **averaging their predictions**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Step-by-Step Working of Bagging Regressor:\n",
        "\n",
        "#### 1. **Bootstrap Sampling**\n",
        "\n",
        "* Create multiple subsets of the training data by **sampling with replacement** (called **bootstrap samples**).\n",
        "* Each subset is typically the same size as the original training data.\n",
        "\n",
        "#### 2. **Train Base Regressors**\n",
        "\n",
        "* Train a **base regressor** (e.g., Decision Tree Regressor) on **each subset**.\n",
        "* Due to different samples, each model learns a slightly different function.\n",
        "\n",
        "#### 3. **Aggregate Predictions**\n",
        "\n",
        "* For a given input, each regressor gives a prediction.\n",
        "* The **final output is the average** of all predictions.\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\frac{1}{M} \\sum_{i=1}^{M} \\hat{y}_i\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $M$ is the number of base models.\n",
        "* $\\hat{y}_i$ is the prediction from the $i^\\text{th}$ model.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Code Example (Using `scikit-learn`):\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Initialize Bagging Regressor\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=10,\n",
        "    oob_score=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"OOB R² Score:\", bagging_regressor.oob_score_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Advantages of Bagging Regressor:\n",
        "\n",
        "| Advantage               | Description                                                    |\n",
        "| ----------------------- | -------------------------------------------------------------- |\n",
        "| Reduces Variance        | Averaging predictions smooths out noise                        |\n",
        "| Improves Generalization | Less overfitting than single models                            |\n",
        "| Handles Non-linearity   | Works well with non-linear base regressors like decision trees |\n",
        "| Robust to Overfitting   | Especially effective with high-variance models                 |\n",
        "\n",
        "---\n",
        "\n",
        "### ❌ Limitations:\n",
        "\n",
        "| Limitation          | Description                          |\n",
        "| ------------------- | ------------------------------------ |\n",
        "| No Bias Reduction   | Does not improve underfitting models |\n",
        "| Slower Training     | Multiple models are trained          |\n",
        "| Harder to Interpret | Combined model is less transparent   |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary Table:\n",
        "\n",
        "| Step        | Action                                                      |\n",
        "| ----------- | ----------------------------------------------------------- |\n",
        "| Sampling    | Draw bootstrap samples from training data                   |\n",
        "| Training    | Train base regressors independently on each sample          |\n",
        "| Aggregation | Average their predictions for final output                  |\n",
        "| Use Case    | Ideal when individual models overfit (e.g., decision trees) |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kGbxWP4f1YaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9.What is the main advantage of ensemble techniques?\n",
        "\n",
        "->### ✅ **Main Advantage of Ensemble Techniques:**\n",
        "\n",
        "The **main advantage** of ensemble techniques is that they **combine multiple models to produce a more accurate, robust, and generalizable prediction** than any single model could achieve on its own.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 **Why Ensembles Work Better:**\n",
        "\n",
        "1. ### 🧩 **Reduced Variance**\n",
        "\n",
        "   * **Bagging** methods like **Random Forest** reduce overfitting by averaging multiple high-variance models.\n",
        "   * This makes predictions more **stable**.\n",
        "\n",
        "2. ### 🧠 **Reduced Bias**\n",
        "\n",
        "   * **Boosting** methods like **Gradient Boosting** combine weak models sequentially, correcting errors step by step.\n",
        "   * Helps reduce **bias**, leading to better learning of complex patterns.\n",
        "\n",
        "3. ### 🔄 **Improved Generalization**\n",
        "\n",
        "   * Ensembles tend to generalize better to unseen data by **balancing bias and variance**.\n",
        "\n",
        "4. ### ⚠️ **Robustness to Noise & Outliers**\n",
        "\n",
        "   * No single model dominates; so, errors due to noise or overfitting are **diluted** across the ensemble.\n",
        "\n",
        "5. ### 🔍 **Better Predictive Performance**\n",
        "\n",
        "   * In practice, ensemble methods consistently outperform individual models in competitions and real-world problems.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Example Comparison:\n",
        "\n",
        "| Model Type           | Accuracy  | Variance | Bias     |\n",
        "| -------------------- | --------- | -------- | -------- |\n",
        "| Single Decision Tree | Medium    | High     | Low      |\n",
        "| Random Forest        | High      | Low      | Low      |\n",
        "| Gradient Boosting    | Very High | Medium   | Very Low |\n",
        "\n",
        "---\n",
        "\n",
        "### 🛠 Common Ensemble Methods:\n",
        "\n",
        "| Technique    | Combines Models By      | Example Models                       |\n",
        "| ------------ | ----------------------- | ------------------------------------ |\n",
        "| **Bagging**  | Parallel averaging      | Random Forest, Bagging Classifier    |\n",
        "| **Boosting** | Sequential correction   | AdaBoost, Gradient Boosting, XGBoost |\n",
        "| **Stacking** | Layered learners        | Stacked Generalization               |\n",
        "| **Voting**   | Majority/average voting | Voting Classifier                    |\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Summary:\n",
        "\n",
        "> **Ensemble techniques improve performance by combining the strengths of multiple models while canceling out their weaknesses.**\n",
        "\n",
        "They help create **strong learners from weak ones** — this is their core power.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MZjrrPFV1oKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10.What is the main challenge of ensemble methods?\n",
        "\n",
        "->### ⚠️ Main Challenge of Ensemble Methods\n",
        "\n",
        "While ensemble methods offer **high accuracy and robustness**, their **main challenge** is **increased complexity** — both in terms of **computation** and **interpretability**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 Key Challenges in Detail:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. 🧠 **Lack of Interpretability**\n",
        "\n",
        "* **Hard to understand and explain** the final prediction.\n",
        "* Unlike simple models (e.g., linear regression), ensembles (like Random Forests or Gradient Boosting) act like black boxes.\n",
        "* This is especially problematic in domains like **medicine**, **finance**, or **law**, where model transparency is critical.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 🖥️ **Computational Cost**\n",
        "\n",
        "* Ensemble methods train **multiple models**, often **dozens or hundreds**.\n",
        "* More training time and **greater memory usage**.\n",
        "* Can be **slow at inference** (prediction) too, especially with large models.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. 🔄 **Complexity in Implementation & Tuning**\n",
        "\n",
        "* More **hyperparameters** to tune (e.g., number of trees, learning rate, max depth).\n",
        "* **Boosting** algorithms like XGBoost or CatBoost require careful tuning to prevent overfitting or underfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. 🧪 **Overfitting Risk (in Boosting)**\n",
        "\n",
        "* Although ensembles are designed to reduce overfitting, **Boosting** (e.g., Gradient Boosting) can **overfit if not regularized properly**.\n",
        "* Especially true with small or noisy datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. 🚀 **Deployment & Maintenance**\n",
        "\n",
        "* Harder to **deploy in production**, especially in real-time systems.\n",
        "* Large model size and inference time can be problematic for mobile or embedded devices.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Quick Summary Table:\n",
        "\n",
        "| Challenge             | Explanation                                      |\n",
        "| --------------------- | ------------------------------------------------ |\n",
        "| **Interpretability**  | Hard to explain decisions made by ensembles      |\n",
        "| **Computation**       | Slower training/inference due to multiple models |\n",
        "| **Tuning Complexity** | Requires careful hyperparameter tuning           |\n",
        "| **Overfitting Risk**  | Especially in boosting methods                   |\n",
        "| **Deployment**        | Heavy models are difficult to scale or deploy    |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Final Thought:\n",
        "\n",
        "> **Ensembles trade off simplicity for performance.**\n",
        "> You get **higher accuracy**, but you often **lose transparency and efficiency**.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FBznF0sD13mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11.Explain the key idea behind ensemble techniques\n",
        "\n",
        "->### 🧠 Key Idea Behind Ensemble Techniques\n",
        "\n",
        "The **key idea** behind ensemble techniques is:\n",
        "\n",
        "> ✅ **\"Combine multiple weak or diverse models to build a strong overall model that performs better than any individual one.\"**\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Why This Works:\n",
        "\n",
        "Just like asking multiple experts can give you a more reliable decision than asking just one, ensemble methods:\n",
        "\n",
        "* **Aggregate diverse opinions (models)**\n",
        "* **Reduce individual errors**\n",
        "* **Balance bias and variance**\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Analogy:\n",
        "\n",
        "> Imagine guessing the number of jellybeans in a jar.\n",
        ">\n",
        "> * One person might be way off.\n",
        "> * But if you average the guesses of 100 people, the result is surprisingly accurate.\n",
        ">   That’s the power of **aggregation** — and that’s the core of ensemble learning.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔄 How Ensembles Work:\n",
        "\n",
        "1. **Train multiple models** — can be same type (e.g., decision trees) or different types (e.g., SVM + Logistic + KNN).\n",
        "2. **Combine their outputs**:\n",
        "\n",
        "   * **Classification**: Use majority voting.\n",
        "   * **Regression**: Take the average.\n",
        "3. Result: A more **accurate and stable** prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Common Ensemble Strategies:\n",
        "\n",
        "| Technique    | Key Idea                                                                | Example            |\n",
        "| ------------ | ----------------------------------------------------------------------- | ------------------ |\n",
        "| **Bagging**  | Train on random subsets, average output                                 | Random Forest      |\n",
        "| **Boosting** | Train sequentially, focus on errors                                     | XGBoost, AdaBoost  |\n",
        "| **Stacking** | Combine different models' outputs using a meta-model                    | Stacked Classifier |\n",
        "| **Voting**   | Combine different classifiers’ predictions via majority or average vote | VotingClassifier   |\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Benefits:\n",
        "\n",
        "* Improves **accuracy**\n",
        "* Reduces **variance** (Bagging)\n",
        "* Reduces **bias** (Boosting)\n",
        "* Increases **robustness** and **generalization**\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary:\n",
        "\n",
        "> **Ensemble = Many Weak Models + Smart Combination = One Strong Model**\n",
        "\n",
        "It's one of the most powerful strategies in machine learning — widely used in both academia and industry.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "R7FYsDxz2FXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12.What is a Random Forest Classifier?\n",
        "\n",
        "->### 🌲 What is a **Random Forest Classifier**?\n",
        "\n",
        "A **Random Forest Classifier** is an **ensemble machine learning model** that combines the predictions of multiple **decision trees** to classify data more accurately and robustly.\n",
        "\n",
        "It uses the principles of **Bagging (Bootstrap Aggregating)** and **Feature Randomness** to reduce overfitting and improve generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 How It Works – Step-by-Step:\n",
        "\n",
        "#### 1. **Bootstrap Sampling (Bagging)**\n",
        "\n",
        "* Create many different subsets of the training data by sampling **with replacement**.\n",
        "* Each tree is trained on a **different bootstrap sample**.\n",
        "\n",
        "#### 2. **Train Decision Trees**\n",
        "\n",
        "* Each tree is trained independently on its sample.\n",
        "* At every split in the tree, only a **random subset of features** is considered (this is **feature randomness**).\n",
        "\n",
        "#### 3. **Majority Voting**\n",
        "\n",
        "* For classification, each tree votes for a class.\n",
        "* The **final prediction** is the **majority vote** across all trees.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Characteristics of Random Forest:\n",
        "\n",
        "| Feature      | Description                              |\n",
        "| ------------ | ---------------------------------------- |\n",
        "| Type         | Ensemble, Supervised Learning            |\n",
        "| Base Learner | Decision Tree                            |\n",
        "| Aggregation  | Majority voting (classification)         |\n",
        "| Handles      | Classification & Regression tasks        |\n",
        "| Key Strength | High accuracy, robustness to overfitting |\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Advantages:\n",
        "\n",
        "* ✔️ **Reduces overfitting** (compared to single decision trees)\n",
        "* ✔️ **Works well with high-dimensional data**\n",
        "* ✔️ **Handles missing values and unbalanced data**\n",
        "* ✔️ **Automatically ranks feature importance**\n",
        "* ✔️ **Parallelizable** (each tree can be trained independently)\n",
        "\n",
        "---\n",
        "\n",
        "### ❌ Disadvantages:\n",
        "\n",
        "* ❌ Slower to train and predict (due to many trees)\n",
        "* ❌ Less interpretable than a single decision tree\n",
        "* ❌ Memory-intensive for large datasets\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Example in Python (Using `scikit-learn`):\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary:\n",
        "\n",
        "> A **Random Forest Classifier** is a powerful, flexible, and widely used model that builds an ensemble of decision trees to make reliable predictions.\n",
        "\n",
        "It’s often a go-to model when you want high performance with minimal tuning.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cjp5Mb132S10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13.What are the main types of ensemble techniques?\n",
        "\n",
        "->### 🧠 Main Types of Ensemble Techniques in Machine Learning\n",
        "\n",
        "Ensemble techniques combine **multiple models** to improve overall performance, accuracy, and robustness. The **main types** are:\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 1. **Bagging (Bootstrap Aggregating)**\n",
        "\n",
        "### 🔧 Key Idea:\n",
        "\n",
        "* Train **multiple models in parallel** on different **bootstrap samples** of the data.\n",
        "* Combine their predictions using **voting (classification)** or **averaging (regression)**.\n",
        "\n",
        "### 📌 Examples:\n",
        "\n",
        "* **Random Forest** (Bagging + Decision Trees)\n",
        "* **BaggingClassifier / BaggingRegressor**\n",
        "\n",
        "### ✅ Benefits:\n",
        "\n",
        "* Reduces **variance**\n",
        "* Helps prevent **overfitting**\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 2. **Boosting**\n",
        "\n",
        "### 🔧 Key Idea:\n",
        "\n",
        "* Train **models sequentially**, each trying to **correct the errors** of the previous one.\n",
        "* Weights are updated to **focus on harder-to-predict samples**.\n",
        "\n",
        "### 📌 Examples:\n",
        "\n",
        "* **AdaBoost**\n",
        "* **Gradient Boosting** (GBM, XGBoost, LightGBM, CatBoost)\n",
        "\n",
        "### ✅ Benefits:\n",
        "\n",
        "* Reduces **bias**\n",
        "* Very **accurate and powerful**, often used in competitions\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 3. **Stacking (Stacked Generalization)**\n",
        "\n",
        "### 🔧 Key Idea:\n",
        "\n",
        "* Train multiple **diverse models** and then use a **meta-model** to combine their predictions.\n",
        "* The base models’ outputs become inputs to the final model.\n",
        "\n",
        "### 📌 Examples:\n",
        "\n",
        "* A stack of Logistic Regression, SVM, and Decision Tree with a meta-model like XGBoost.\n",
        "\n",
        "### ✅ Benefits:\n",
        "\n",
        "* Leverages the **strengths of different algorithms**\n",
        "* Often achieves **better performance** than individual models\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 4. **Voting**\n",
        "\n",
        "### 🔧 Key Idea:\n",
        "\n",
        "* Combine predictions from **different classifiers** by:\n",
        "\n",
        "  * **Majority vote** (hard voting)\n",
        "  * **Average of predicted probabilities** (soft voting)\n",
        "\n",
        "### 📌 Examples:\n",
        "\n",
        "* Combine SVM, KNN, and Logistic Regression using `VotingClassifier` in `sklearn`.\n",
        "\n",
        "### ✅ Benefits:\n",
        "\n",
        "* Simple to implement\n",
        "* Works well if base models are diverse and fairly accurate\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 5. **Blending** (Variant of Stacking)\n",
        "\n",
        "### 🔧 Key Idea:\n",
        "\n",
        "* Similar to stacking, but uses a **validation set** (not cross-validation) to train the meta-model.\n",
        "* Slightly simpler but less data-efficient than stacking.\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Summary Table\n",
        "\n",
        "| Ensemble Type | Trained Models | Combination Method      | Main Goal          | Example               |\n",
        "| ------------- | -------------- | ----------------------- | ------------------ | --------------------- |\n",
        "| **Bagging**   | Parallel       | Majority vote / average | Reduce variance    | Random Forest         |\n",
        "| **Boosting**  | Sequential     | Weighted sum            | Reduce bias        | XGBoost, AdaBoost     |\n",
        "| **Stacking**  | Parallel       | Meta-learner            | Leverage diversity | Logistic + SVM + Tree |\n",
        "| **Voting**    | Parallel       | Vote / probability avg  | Simplicity         | VotingClassifier      |\n",
        "| **Blending**  | Parallel       | Meta-model (on val set) | Simpler stacking   | Custom ensemble       |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Final Thought:\n",
        "\n",
        "> **Different ensemble methods solve different problems** — bagging fights overfitting, boosting fixes underfitting, stacking boosts power by mixing models.\n",
        "\n"
      ],
      "metadata": {
        "id": "YmZBwsH22jZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##14.What is ensemble learning in machine learning?\n",
        "\n",
        "->### 🧠 What is Ensemble Learning in Machine Learning?\n",
        "\n",
        "**Ensemble learning** is a machine learning technique that **combines multiple models (often called \"base learners\")** to solve a problem and improve predictive performance compared to any individual model.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 **Core Idea:**\n",
        "\n",
        "> **“Many weak models together can form a strong model.”**\n",
        "\n",
        "By aggregating the predictions of several models, ensemble learning:\n",
        "\n",
        "* Increases **accuracy**\n",
        "* Reduces **overfitting** and **variance**\n",
        "* Improves **generalization** on unseen data\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Real-World Analogy:\n",
        "\n",
        "Think of it like **taking multiple expert opinions** before making a decision. One expert may be wrong, but if most of them agree, the final decision is more likely to be correct.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧱 Types of Ensemble Learning Techniques:\n",
        "\n",
        "| Technique    | Key Concept                                                                    | Example Algorithms                   |\n",
        "| ------------ | ------------------------------------------------------------------------------ | ------------------------------------ |\n",
        "| **Bagging**  | Train multiple models in parallel on random subsets of data (with replacement) | Random Forest                        |\n",
        "| **Boosting** | Train models sequentially, each focusing on previous errors                    | AdaBoost, Gradient Boosting, XGBoost |\n",
        "| **Stacking** | Combine predictions from different models using a meta-model                   | Stacked Generalization               |\n",
        "| **Voting**   | Aggregate predictions by majority (classification) or average (regression)     | Voting Classifier                    |\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Benefits of Ensemble Learning:\n",
        "\n",
        "| Benefit                | Description                                 |\n",
        "| ---------------------- | ------------------------------------------- |\n",
        "| 🎯 **Better Accuracy** | Outperforms individual models in most cases |\n",
        "| 🛡️ **Robustness**     | Less sensitive to noise and overfitting     |\n",
        "| 🔄 **Generalization**  | Performs better on unseen data              |\n",
        "| 🔍 **Versatility**     | Can combine different types of models       |\n",
        "\n",
        "---\n",
        "\n",
        "### ❌ Challenges:\n",
        "\n",
        "| Challenge              | Explanation                                       |\n",
        "| ---------------------- | ------------------------------------------------- |\n",
        "| 🧠 Less Interpretable  | Hard to explain decisions of many combined models |\n",
        "| 🖥️ Computational Cost | Requires more time and memory                     |\n",
        "| 🔧 More Tuning Needed  | More hyperparameters and architecture decisions   |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary:\n",
        "\n",
        "> **Ensemble learning = smarter decisions by combining multiple learners.**\n",
        "\n",
        "It is widely used in practice and **dominates machine learning competitions** (like Kaggle), often being the key to winning solutions.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dqmWRVwk2wYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##15.When should we avoid using ensemble methods?\n",
        "\n",
        "->Great question. While **ensemble methods** are powerful, they are **not always the best choice**. Here’s a breakdown of situations where you might want to **avoid using ensemble methods**:\n",
        "\n",
        "---\n",
        "\n",
        "### 🚫 When to Avoid Ensemble Methods:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. 🧠 **You Need Model Interpretability**\n",
        "\n",
        "* If you need a model that is **simple, explainable, and easy to understand** (e.g., in healthcare, finance, or legal decisions), ensemble methods (like Random Forest, XGBoost) may be too **opaque** or **hard to interpret**.\n",
        "* ✅ Prefer: **Logistic Regression**, **Decision Trees**, or **Rule-based models**\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ⚡ **You Need Fast Training or Prediction**\n",
        "\n",
        "* Ensemble methods (especially boosting) can be **computationally expensive**.\n",
        "* If you’re working with **limited hardware**, **real-time predictions**, or **very large datasets**, ensembles might be **too slow or resource-heavy**.\n",
        "* ✅ Prefer: **Naïve Bayes**, **Linear Models**, or small **Decision Trees**\n",
        "\n",
        "---\n",
        "\n",
        "### 3. 📉 **Your Base Model Already Performs Well**\n",
        "\n",
        "* If a **simple model gives high accuracy** and generalizes well, adding ensemble complexity may **not yield significant gains**—but will increase training cost and code complexity.\n",
        "* ✅ Tip: Always benchmark against a **baseline model** first.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. 🔍 **Small or Very Clean Datasets**\n",
        "\n",
        "* Ensemble methods shine when data is **noisy**, **complex**, or has **high variance**.\n",
        "* On a **small or very clean dataset**, a single well-tuned model might outperform or match an ensemble.\n",
        "* ✅ Prefer: A single **decision tree**, **SVM**, or **logistic regression**.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. 🧪 **Experimental or Educational Scenarios**\n",
        "\n",
        "* If you are learning or demonstrating machine learning concepts (like overfitting, bias-variance trade-off), ensembles may **hide the fundamental behavior** of individual models.\n",
        "* ✅ Prefer: Simple models for clarity\n",
        "\n",
        "---\n",
        "\n",
        "### 6. ⚙️ **Deployment Constraints**\n",
        "\n",
        "* Ensembles often require **more memory**, **larger file sizes**, and **slower prediction times**.\n",
        "* This can be a problem in environments like **mobile apps**, **embedded systems**, or **edge devices**.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Summary Table:\n",
        "\n",
        "| Avoid Ensembles When...                | Because...                                    |\n",
        "| -------------------------------------- | --------------------------------------------- |\n",
        "| You need interpretability              | Ensembles are black-box models                |\n",
        "| You need speed or have hardware limits | Ensembles are computationally expensive       |\n",
        "| A simple model performs well           | Ensemble adds complexity with minimal benefit |\n",
        "| The dataset is small or clean          | Overkill; simpler models may be sufficient    |\n",
        "| You’re learning basic ML concepts      | Ensembles obscure core learning behaviors     |\n",
        "| You need lightweight deployment        | Ensemble models are heavier and slower        |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Final Tip:\n",
        "\n",
        "> **Start simple. Use ensembles only when you need more performance.**\n",
        "\n"
      ],
      "metadata": {
        "id": "-DT3lQG12_jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "->### 🎯 How Does **Bagging Help Reduce Overfitting?**\n",
        "\n",
        "**Bagging** (Bootstrap Aggregating) helps reduce **overfitting** by lowering the **variance** of high-variance models—especially decision trees.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Overfitting Problem Recap:\n",
        "\n",
        "* Overfitting occurs when a model **memorizes noise or small fluctuations** in the training data, rather than learning the true underlying patterns.\n",
        "* Models like **decision trees** are highly flexible and prone to overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ How Bagging Solves This:\n",
        "\n",
        "#### 🔹 1. **Bootstrap Sampling (Random Subsets)**\n",
        "\n",
        "* Bagging trains each model on a **random sample of the data (with replacement)**.\n",
        "* This introduces **diversity** among the models, because each one sees a **slightly different dataset**.\n",
        "\n",
        "#### 🔹 2. **Training Multiple Models (Independent Learners)**\n",
        "\n",
        "* Bagging builds **multiple models in parallel**.\n",
        "* Each one may overfit **its own training subset**, but in **different ways**.\n",
        "\n",
        "#### 🔹 3. **Aggregation of Predictions**\n",
        "\n",
        "* For classification: use **majority voting**\n",
        "* For regression: use **averaging**\n",
        "\n",
        "👉 **Averaging smooths out the “spiky” predictions** of individual overfitted models.\n",
        "\n",
        "> 🔑 The ensemble is less sensitive to the noise or outliers that any one model may overfit.\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Visual Intuition:\n",
        "\n",
        "| Model Type                   | Prediction Behavior                                     |\n",
        "| ---------------------------- | ------------------------------------------------------- |\n",
        "| Single Decision Tree         | May create deep, complex splits                         |\n",
        "| Bagged Trees (Random Forest) | Combines many diverse trees → more stable, less overfit |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Example: Decision Tree vs Random Forest\n",
        "\n",
        "| Metric            | Single Decision Tree | Bagging (Random Forest) |\n",
        "| ----------------- | -------------------- | ----------------------- |\n",
        "| Training Accuracy | Very High (overfits) | High                    |\n",
        "| Test Accuracy     | Lower                | Higher (less overfit)   |\n",
        "| Variance          | High                 | Reduced                 |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary:\n",
        "\n",
        "> **Bagging reduces overfitting by lowering variance through model averaging.**\n",
        "\n",
        "| Technique | Reduces Overfitting? | How?                                      |\n",
        "| --------- | -------------------- | ----------------------------------------- |\n",
        "| Bagging   | ✅ Yes                | Aggregates diverse overfit models         |\n",
        "| Boosting  | ⚠️ Sometimes         | Focuses on bias, can overfit if unchecked |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7HBXHTzQ3Lz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "->### 🌲 Why is **Random Forest** Better Than a Single **Decision Tree**?\n",
        "\n",
        "A **Random Forest** is usually better than a single **Decision Tree** because it corrects the biggest weakness of decision trees: **high variance and overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Key Reasons:\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 1. **Reduces Overfitting**\n",
        "\n",
        "* **Decision Trees** are prone to **overfitting**, especially on noisy or complex datasets.\n",
        "* **Random Forest** builds **many trees** and combines their results, making the final model **more generalizable**.\n",
        "\n",
        "> ➤ Random Forest averages out the errors and variance from individual trees.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 2. **Increases Accuracy**\n",
        "\n",
        "* Random Forest often achieves **higher test accuracy** because it's more robust and stable.\n",
        "* It captures more reliable patterns without being overly influenced by noise.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 3. **Reduces Variance**\n",
        "\n",
        "* A single tree might perform very differently with different data splits.\n",
        "* Random Forest reduces this **variance** by combining the output of many trees trained on different bootstrap samples.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 4. **Handles Feature Importance**\n",
        "\n",
        "* Random Forest gives you a measure of **feature importance**, helping you understand which inputs are most influential.\n",
        "* This is not naturally available in a single decision tree.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 5. **Robust to Noise and Outliers**\n",
        "\n",
        "* Outliers can significantly affect a single tree.\n",
        "* In a Random Forest, **outliers affect only some trees**, not the entire ensemble.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 6. **Better Generalization**\n",
        "\n",
        "* Decision Trees can memorize training data (high training accuracy, low test accuracy).\n",
        "* Random Forest uses **averaging (or voting)** to generalize better to unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Quick Comparison:\n",
        "\n",
        "| Feature                  | Decision Tree             | Random Forest          |\n",
        "| ------------------------ | ------------------------- | ---------------------- |\n",
        "| **Overfitting Risk**     | High                      | Low                    |\n",
        "| **Accuracy**             | Medium                    | High                   |\n",
        "| **Variance**             | High                      | Reduced                |\n",
        "| **Bias**                 | Low                       | Slightly higher        |\n",
        "| **Interpretability**     | Easy to interpret         | Harder to interpret    |\n",
        "| **Prediction Stability** | Sensitive to data changes | Stable across datasets |\n",
        "| **Handles Noise**        | Poorly                    | Robust                 |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary:\n",
        "\n",
        "> **Random Forest = Many Trees + Bootstrapping + Feature Randomness = Strong, Stable Model**\n",
        "\n",
        "It combines the strengths of many weak learners (trees) to form a **powerful, less overfit, and more accurate model**.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bDiUOgaX3aK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##18.What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "->### 🧪 What is the Role of **Bootstrap Sampling** in Bagging?\n",
        "\n",
        "**Bootstrap sampling** is a core component of **Bagging (Bootstrap Aggregating)** — it plays a crucial role in making the ensemble diverse and effective.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧾 Definition:\n",
        "\n",
        "**Bootstrap sampling** is the process of creating **random subsets** of the training data by **sampling with replacement**.\n",
        "\n",
        "* Each subset (called a **bootstrap sample**) has the **same size** as the original dataset.\n",
        "* Because of replacement, some samples appear **multiple times**, and others may be **left out**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Role in Bagging:\n",
        "\n",
        "| 🔍 Purpose                   | 🧠 Explanation                                                                                                                                             |\n",
        "| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| ✅ **Introduces Diversity**   | Each model in the ensemble sees a **different training dataset**, so it learns different patterns.                                                         |\n",
        "| ✅ **Reduces Variance**       | The randomness from bootstrap sampling causes the individual models to vary; averaging them **smooths out fluctuations**.                                  |\n",
        "| ✅ **Prevents Overfitting**   | Because each model overfits **differently**, their combined prediction is more **generalizable**.                                                          |\n",
        "| ✅ **Enables OOB Evaluation** | \\~37% of data points are **out-of-bag** (not included in a sample), which can be used to estimate model performance **without a separate validation set**. |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Example in Python (Bootstrapping a dataset):\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Original dataset\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([10, 20, 30, 40, 50])\n",
        "\n",
        "# Bootstrap sample\n",
        "X_sample, y_sample = resample(X, y, replace=True, n_samples=5, random_state=42)\n",
        "\n",
        "print(\"Original X:\", X)\n",
        "print(\"Bootstrap Sample X:\", X_sample)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Summary Table:\n",
        "\n",
        "| Aspect                 | With Bootstrap Sampling | Without Bootstrap Sampling |\n",
        "| ---------------------- | ----------------------- | -------------------------- |\n",
        "| Model Diversity        | High                    | Low                        |\n",
        "| Variance Reduction     | Yes                     | Less effective             |\n",
        "| Overfitting Protection | Strong                  | Weaker                     |\n",
        "| OOB Score Available    | Yes                     | No                         |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Final Thought:\n",
        "\n",
        "> **Bootstrap sampling** is what gives Bagging its strength: diversity.\n",
        "> By making each model see a slightly different version of the data, Bagging creates a strong, stable ensemble.\n",
        "\n"
      ],
      "metadata": {
        "id": "yWLAXIYx3nEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##19.What are some real-world applications of ensemble techniques?\n",
        "\n",
        "->### 🌍 Real-World Applications of Ensemble Techniques\n",
        "\n",
        "Ensemble methods are widely used in real-world machine learning tasks because they provide **high accuracy, robustness**, and **generalization**. Here are some common domains where ensembles shine:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. 🏦 **Finance**\n",
        "\n",
        "* **Credit scoring**: Predict loan default risk more accurately.\n",
        "* **Fraud detection**: Detect suspicious transactions using ensembles like Random Forests or XGBoost.\n",
        "* **Stock market prediction**: Combine weak models for better forecasting.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 🏥 **Healthcare**\n",
        "\n",
        "* **Disease diagnosis**: Classify diseases (e.g., cancer detection from images or patient data) using ensemble models like Gradient Boosting.\n",
        "* **Medical image analysis**: Segment and detect abnormalities using stacked CNN ensembles.\n",
        "* **Patient risk prediction**: Predict ICU admission or readmission using ensemble decision trees.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. 🎯 **Marketing and Sales**\n",
        "\n",
        "* **Customer churn prediction**: Use Random Forests or Gradient Boosting to identify which customers are likely to leave.\n",
        "* **Recommendation systems**: Blend multiple algorithms for better product recommendations (used in e-commerce platforms).\n",
        "* **Sales forecasting**: Combine regressors for demand prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. 📱 **Text and Language Processing**\n",
        "\n",
        "* **Spam detection**: Combine Naive Bayes, Logistic Regression, and SVM using Voting or Stacking.\n",
        "* **Sentiment analysis**: Boosting models like XGBoost often perform well on text classification.\n",
        "* **Chatbots and NLP tasks**: Ensemble transformers or LSTM-based models for better understanding.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. 🛡️ **Cybersecurity**\n",
        "\n",
        "* **Intrusion detection systems**: Use ensemble methods to classify network activity as normal or malicious.\n",
        "* **Malware classification**: Detect and classify different types of malware.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. 📷 **Computer Vision**\n",
        "\n",
        "* **Image classification**: Combine CNNs or decision trees for more robust predictions.\n",
        "* **Face recognition**: Ensemble deep learning models for identity verification.\n",
        "* **Object detection**: Use ensemble bounding box predictors in real-time applications (e.g., autonomous driving).\n",
        "\n",
        "---\n",
        "\n",
        "### 7. 🚗 **Autonomous Vehicles**\n",
        "\n",
        "* **Sensor fusion**: Ensemble models process LIDAR, camera, and radar data.\n",
        "* **Path planning and decision-making**: Combine multiple models to reduce risk of error.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. 🧪 **Scientific Research**\n",
        "\n",
        "* **Bioinformatics**: Predict gene expression or protein folding.\n",
        "* **Climate modeling**: Combine various predictive models for more accurate forecasting.\n",
        "* **Drug discovery**: Use ensembles to predict molecule activity or toxicity.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. 🧠 **Kaggle Competitions and ML Challenges**\n",
        "\n",
        "* Most winning solutions involve:\n",
        "\n",
        "  * **Stacking** multiple models\n",
        "  * **Blending** tree-based models and neural nets\n",
        "  * **Averaging** predictions from hundreds of models\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary\n",
        "\n",
        "| Domain          | Application Example                      | Ensemble Methods Used             |\n",
        "| --------------- | ---------------------------------------- | --------------------------------- |\n",
        "| Finance         | Credit scoring, fraud detection          | Random Forest, XGBoost            |\n",
        "| Healthcare      | Diagnosis, image classification          | Gradient Boosting, Stacking       |\n",
        "| Marketing       | Churn prediction, recommendation systems | Voting, Bagging, Blending         |\n",
        "| NLP             | Sentiment analysis, spam filtering       | Voting, Boosting, Stacking        |\n",
        "| Cybersecurity   | Intrusion/malware detection              | Random Forest, AdaBoost           |\n",
        "| Vision          | Face recognition, object detection       | CNN + ensemble, Gradient Boosting |\n",
        "| Autonomous Cars | Path planning, sensor fusion             | Ensemble CNNs, Stacking           |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "w8eBkJJ93zKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##20.What is the difference between Bagging and Boosting?\n",
        "\n",
        "->### 🔍 Difference Between **Bagging** and **Boosting**\n",
        "\n",
        "Both **Bagging** and **Boosting** are ensemble learning techniques that improve model performance by combining multiple learners — but they do it in **very different ways**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Side-by-Side Comparison:\n",
        "\n",
        "| Aspect                   | **Bagging**                                    | **Boosting**                                                 |\n",
        "| ------------------------ | ---------------------------------------------- | ------------------------------------------------------------ |\n",
        "| 📌 **Goal**              | Reduce **variance** (overfitting)              | Reduce **bias** (underfitting)                               |\n",
        "| 🔁 **Model Training**    | Models trained **in parallel**                 | Models trained **sequentially**                              |\n",
        "| 🔀 **Sampling**          | Uses **bootstrap sampling** (with replacement) | No sampling; each model focuses on previous errors           |\n",
        "| 🎯 **Focus**             | Each model learns **independently**            | Each new model corrects the **mistakes** of the previous one |\n",
        "| 🧠 **Model Type**        | All models are given **equal weight**          | Later models are given **more weight**                       |\n",
        "| 🛡️ **Overfitting Risk** | Lower (especially with high-variance models)   | Higher if not tuned properly                                 |\n",
        "| ⚡ **Training Time**      | Faster (parallelizable)                        | Slower (must run sequentially)                               |\n",
        "| 📈 **Performance**       | Good baseline model                            | Often achieves **state-of-the-art** performance              |\n",
        "| 📊 **Examples**          | Random Forest, BaggingClassifier               | AdaBoost, Gradient Boosting, XGBoost                         |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Intuition:\n",
        "\n",
        "* **Bagging** = \"Train many independent learners on random data and average their predictions.\"\n",
        "* **Boosting** = \"Train a sequence of learners, where each tries to fix the errors of the one before it.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Code Illustration (Scikit-learn):\n",
        "\n",
        "**Bagging:**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "model = BaggingClassifier()\n",
        "```\n",
        "\n",
        "**Boosting:**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary in One Line:\n",
        "\n",
        "> ✅ **Bagging** reduces variance by averaging many overfit models.\n",
        "> ✅ **Boosting** reduces bias by focusing on errors and building a strong model step-by-step.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6_6Ux4B4B_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "MrkDEm794Mcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21.Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the sample dataset (Iris)\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the base estimator (Decision Tree)\n",
        "base_model = DecisionTreeClassifier()\n",
        "\n",
        "# Define the Bagging Classifier\n",
        "bagging_model = BaggingClassifier(estimator=base_model, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "tvESVTh84PeY",
        "outputId": "7e960304-2d4f-40e8-e3db-fef7dbb22add"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-1440479728.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Define the Bagging Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mbagging_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Train the Bagging Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load a sample regression dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the base regressor (Decision Tree)\n",
        "base_regressor = DecisionTreeRegressor()\n",
        "\n",
        "# Create the Bagging Regressor (Ensemble Learning)\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    estimator=base_regressor,\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the ensemble model\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Ed5W2QHE4azZ",
        "outputId": "c983a76d-3b71-4299-b679-e06e6fb0e250"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-757752068.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Create the Bagging Regressor (Ensemble Learning)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m bagging_regressor = BaggingRegressor(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_regressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importance scores\n",
        "print(feature_importance_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DtO2EOf7bJo",
        "outputId": "f339caa1-f055-4dc4-d6d6-9f8301da90fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Feature  Importance\n",
            "7       mean concave points    0.141934\n",
            "27     worst concave points    0.127136\n",
            "23               worst area    0.118217\n",
            "6            mean concavity    0.080557\n",
            "20             worst radius    0.077975\n",
            "22          worst perimeter    0.074292\n",
            "2            mean perimeter    0.060092\n",
            "3                 mean area    0.053810\n",
            "26          worst concavity    0.041080\n",
            "0               mean radius    0.032312\n",
            "13               area error    0.029538\n",
            "21            worst texture    0.018786\n",
            "25        worst compactness    0.017539\n",
            "10             radius error    0.016435\n",
            "28           worst symmetry    0.012929\n",
            "12          perimeter error    0.011770\n",
            "24         worst smoothness    0.011769\n",
            "1              mean texture    0.011064\n",
            "5          mean compactness    0.009216\n",
            "19  fractal dimension error    0.007135\n",
            "29  worst fractal dimension    0.006924\n",
            "4           mean smoothness    0.006223\n",
            "14         smoothness error    0.005881\n",
            "16          concavity error    0.005816\n",
            "15        compactness error    0.004596\n",
            "18           symmetry error    0.004001\n",
            "17     concave points error    0.003382\n",
            "8             mean symmetry    0.003278\n",
            "11            texture error    0.003172\n",
            "9    mean fractal dimension    0.003140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24.Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Regressor\n",
        "tree_model = DecisionTreeRegressor(random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_preds = tree_model.predict(X_test)\n",
        "tree_mse = mean_squared_error(y_test, tree_preds)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "\n",
        "# Print and compare results\n",
        "print(\"Decision Tree MSE:\", tree_mse)\n",
        "print(\"Random Forest MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZjsy27n79eb",
        "outputId": "00fa140a-6ded-4d70-9040-7964b199101e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 0.5280096503174904\n",
            "Random Forest MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data (optional for comparison, not needed for OOB itself)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Random Forest with OOB enabled\n",
        "rf_model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "# Train on training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Print the OOB score\n",
        "print(\"Out-of-Bag (OOB) Score:\", rf_model.oob_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woJL51pm8OBa",
        "outputId": "96ad4982-447f-4b7e-81db-2dbb805f773c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag (OOB) Score: 0.9547738693467337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#26.Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the base estimator (Support Vector Classifier)\n",
        "svm = SVC(probability=True, kernel='rbf', random_state=42)\n",
        "\n",
        "# Create Bagging Classifier with SVM as base estimator\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=svm,\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier with SVM Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "kLqKpCke8blZ",
        "outputId": "a4914290-1742-49ae-e5e1-aa8f3987b364"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-474529645.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Create Bagging Classifier with SVM as base estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m bagging_model = BaggingClassifier(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#27.Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define different tree counts to test\n",
        "tree_counts = [1, 5, 10, 50, 100, 200]\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate for each tree count\n",
        "for n in tree_counts:\n",
        "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"n_estimators = {n}: Accuracy = {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p16Vzrp8lsr",
        "outputId": "cf213040-7c5d-410f-9871-14e4ac95a1ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_estimators = 1: Accuracy = 0.9474\n",
            "n_estimators = 5: Accuracy = 0.9649\n",
            "n_estimators = 10: Accuracy = 0.9649\n",
            "n_estimators = 50: Accuracy = 0.9708\n",
            "n_estimators = 100: Accuracy = 0.9708\n",
            "n_estimators = 200: Accuracy = 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#28.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset (binary classification)\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimator\n",
        "base_model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "\n",
        "# Define Bagging Classifier\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=base_model,\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC AUC\n",
        "y_proba = bagging_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = roc_auc_score(y_test, y_proba)\n",
        "print(\"AUC Score (Bagging + Logistic Regression):\", auc_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "4XzVpjFL83lh",
        "outputId": "391a4aa9-9b2d-4a17-f6ff-25dae03b263a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-2748884726.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Define Bagging Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m bagging_model = BaggingClassifier(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#29.Train a Random Forest Regressor and analyze feature importance scores\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "feature_names = X.columns\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Optional: plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=feature_importance_df, x='Importance', y='Feature', palette='viridis')\n",
        "plt.title('Random Forest Regressor - Feature Importances')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "3ER94Yww8-ls",
        "outputId": "408ba16c-77af-43a2-e984-317cd5f79966"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "      Feature  Importance\n",
            "0      MedInc    0.526011\n",
            "5    AveOccup    0.138220\n",
            "7   Longitude    0.086124\n",
            "6    Latitude    0.086086\n",
            "1    HouseAge    0.054654\n",
            "2    AveRooms    0.047188\n",
            "4  Population    0.031722\n",
            "3   AveBedrms    0.029995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-267366187.py:35: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(data=feature_importance_df, x='Importance', y='Feature', palette='viridis')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#30.Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1️⃣ Bagging Classifier (with Decision Tree base)\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_preds = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "# 2️⃣ Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
        "\n",
        "# Print comparison\n",
        "print(f\"Bagging Classifier Accuracy:       {bagging_accuracy:.4f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "N4uW4utq9M4R",
        "outputId": "f3835269-37cd-4887-e7dc-d377e7c8e3fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-1797005644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 1️⃣ Bagging Classifier (with Decision Tree base)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m bagging_model = BaggingClassifier(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Set Accuracy with Best Model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcMy3Oiw9Yzg",
        "outputId": "a298aac2-c5d7-4be0-824a-822aada1e489"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Test Set Accuracy with Best Model: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#32.Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of different numbers of estimators to test\n",
        "n_estimators_list = [1, 5, 10, 50, 100, 200]\n",
        "mse_scores = []\n",
        "\n",
        "# Train and evaluate BaggingRegressor for each value of n_estimators\n",
        "for n in n_estimators_list:\n",
        "    model = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"n_estimators = {n}: MSE = {mse:.4f}\")\n",
        "\n",
        "# Optional: Plot the results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(n_estimators_list, mse_scores, marker='o', linestyle='--')\n",
        "plt.xlabel(\"Number of Base Estimators\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title(\"Performance of Bagging Regressor with Varying Estimators\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "rj3GsM_-9YkK",
        "outputId": "24501ab0-11b2-4d72-9d34-eb01a297fe77"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-3233035039.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Train and evaluate BaggingRegressor for each value of n_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_estimators_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     model = BaggingRegressor(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#33.Train a Random Forest Classifier and analyze misclassified samples\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "target_names = data.target_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_indices = (y_pred != y_test)\n",
        "misclassified_samples = X_test[misclassified_indices]\n",
        "true_labels = y_test[misclassified_indices]\n",
        "predicted_labels = y_pred[misclassified_indices]\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "misclassified_df = pd.DataFrame(misclassified_samples, columns=feature_names)\n",
        "misclassified_df['True Label'] = [target_names[i] for i in true_labels]\n",
        "misclassified_df['Predicted Label'] = [target_names[i] for i in predicted_labels]\n",
        "\n",
        "# Show misclassified samples\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(misclassified_df.head())\n"
      ],
      "metadata": {
        "id": "ffxDjE-k9fRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34.Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1️⃣ Single Decision Tree Classifier\n",
        "tree_model = DecisionTreeClassifier(random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_preds = tree_model.predict(X_test)\n",
        "tree_accuracy = accuracy_score(y_test, tree_preds)\n",
        "\n",
        "# 2️⃣ Bagging Classifier using Decision Tree as base estimator\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_preds = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "# 📊 Compare Results\n",
        "print(f\"Single Decision Tree Accuracy: {tree_accuracy:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy:   {bagging_accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "fC_xkNs_9fB3",
        "outputId": "6a43d5ae-0b3a-4000-c601-7f139748784f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-16-738656544.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 2️⃣ Bagging Classifier using Decision Tree as base estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m bagging_model = BaggingClassifier(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#35.Train a Random Forest Classifier and visualize the confusion matrix\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot using seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vosBVQBv9eyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36.Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base learners\n",
        "base_learners = [\n",
        "    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(kernel='rbf', probability=True, random_state=42))\n",
        "]\n",
        "\n",
        "# Define final estimator\n",
        "final_estimator = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Build stacking classifier\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=final_estimator,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_stack = stacking_model.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Fit and evaluate individual models\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_accuracy = accuracy_score(y_test, dt_model.predict(X_test))\n",
        "\n",
        "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_accuracy = accuracy_score(y_test, svm_model.predict(X_test))\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Decision Tree Accuracy:    {dt_accuracy:.4f}\")\n",
        "print(f\"SVM Accuracy:              {svm_accuracy:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "bPpxMMg5-fvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37.Train a Random Forest Classifier and print the top 5 most important features\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create DataFrame for sorting\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(importance_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqmbvpa4-fkS",
        "outputId": "de2c3501-6660-4345-af3f-5bf500048c80"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "7    mean concave points    0.141934\n",
            "27  worst concave points    0.127136\n",
            "23            worst area    0.118217\n",
            "6         mean concavity    0.080557\n",
            "20          worst radius    0.077975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#38.Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define and train Bagging Classifier\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=['malignant', 'benign']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "eOJqx7qS-fYU",
        "outputId": "b125a9e5-8f6f-4ede-9c2e-e7a679110b79"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-2247330722.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Define and train Bagging Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m bagging_model = BaggingClassifier(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#39.Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Test different max_depth values\n",
        "depths = [1, 2, 3, 5, 10, 15, 20, 30, None]\n",
        "accuracies = []\n",
        "\n",
        "for depth in depths:\n",
        "    model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"max_depth = {depth}: Accuracy = {acc:.4f}\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "depth_labels = [str(d) if d is not None else \"None\" for d in depths]\n",
        "plt.plot(depth_labels, accuracies, marker='o', linestyle='--', color='blue')\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of max_depth on Random Forest Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "LmswRRIP-fIX",
        "outputId": "84716303-0784-4d27-bf2c-a969af746978"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_depth = 1: Accuracy = 0.9591\n",
            "max_depth = 2: Accuracy = 0.9532\n",
            "max_depth = 3: Accuracy = 0.9708\n",
            "max_depth = 5: Accuracy = 0.9649\n",
            "max_depth = 10: Accuracy = 0.9708\n",
            "max_depth = 15: Accuracy = 0.9708\n",
            "max_depth = 20: Accuracy = 0.9708\n",
            "max_depth = 30: Accuracy = 0.9708\n",
            "max_depth = None: Accuracy = 0.9708\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#40Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Base Estimator 1: Decision Tree Regressor\n",
        "bagging_tree = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_tree.fit(X_train, y_train)\n",
        "y_pred_tree = bagging_tree.predict(X_test)\n",
        "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
        "\n",
        "# Base Estimator 2: K-Neighbors Regressor\n",
        "bagging_knn = BaggingRegressor(\n",
        "    base_estimator=KNeighborsRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "# Print results\n",
        "print(f\"Bagging Regressor (Decision Tree) MSE: {mse_tree:.4f}\")\n",
        "print(f\"Bagging Regressor (KNeighbors)   MSE: {mse_knn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "3PtH59nW-dPx",
        "outputId": "f735f41c-4766-4ac6-d737-d9045db8a80c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-4015536028.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Base Estimator 1: Decision Tree Regressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m bagging_tree = BaggingRegressor(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#41.Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict class probabilities\n",
        "y_proba = rf.predict_proba(X_test)[:, 1]  # Probability of class 1 (benign)\n",
        "\n",
        "# Compute ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Print result\n",
        "print(f\"Random Forest Classifier ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "VWhdlRvN_YIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42.Train a Bagging Classifier and evaluate its performance using cross-validatio.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Define Bagging Classifier with Decision Tree base\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Evaluate using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(bagging_model, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-Validation Accuracy Scores:\", np.round(cv_scores, 4))\n",
        "print(\"Mean Accuracy: {:.4f}\".format(np.mean(cv_scores)))\n",
        "print(\"Standard Deviation: {:.4f}\".format(np.std(cv_scores)))\n"
      ],
      "metadata": {
        "id": "nhPgXqba_X94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43.Train a Random Forest Classifier and plot the Precision-Recall curves\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall pairs and average precision\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall, precision, label=f'Random Forest (AP = {avg_precision:.2f})', color='blue')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Random Forest Classifier')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "faFrw89n_Xx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44.Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Define final estimator\n",
        "final_estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Build stacking classifier\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=final_estimator,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train stacking model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "stacking_preds = stacking_model.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_preds)\n",
        "\n",
        "# Train and evaluate individual models\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_accuracy = accuracy_score(y_test, rf_model.predict(X_test))\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_accuracy = accuracy_score(y_test, lr_model.predict(X_test))\n",
        "\n",
        "# Print results\n",
        "print(f\"Logistic Regression Accuracy:     {lr_accuracy:.4f}\")\n",
        "print(f\"Random Forest Accuracy:           {rf_accuracy:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy:     {stacking_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "NNxrZEx__XlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45.Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Split into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define different bootstrap sample sizes (as a fraction of training data)\n",
        "max_samples_list = [0.3, 0.5, 0.7, 1.0]\n",
        "mse_scores = []\n",
        "\n",
        "# Train and evaluate models\n",
        "for max_samples in max_samples_list:\n",
        "    model = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=100,\n",
        "        max_samples=max_samples,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"max_samples = {max_samples:.1f}: MSE = {mse:.4f}\")\n",
        "\n",
        "# Plot MSE vs max_samples\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(max_samples_list, mse_scores, marker='o', linestyle='--', color='green')\n",
        "plt.xlabel(\"max_samples (fraction of training data)\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title(\"Bagging Regressor Performance vs Bootstrap Sample Size\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GJ8KCPP8_Wk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}